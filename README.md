# Finding Universal Dependency patterns in multilingual BERTâ€™s self-attention mechanisms

This work is based on the paper [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341) and 
the code available on the [github repository](https://github.com/clarkkev/attention-analysis) of the paper.

The procedure that is used to replicate this work is:
1. Train Bert
2. Extract attention heads
3. Analyse using the script [Language_Analysis.py](./Language_Analysis.py)
